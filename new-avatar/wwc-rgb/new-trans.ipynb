{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm==0.4.12 in /home/srikanth/graspenv/lib/python3.10/site-packages (0.4.12)\n",
      "Requirement already satisfied: torch>=1.4 in /home/srikanth/graspenv/lib/python3.10/site-packages (from timm==0.4.12) (2.0.0+cu118)\n",
      "Requirement already satisfied: torchvision in /home/srikanth/graspenv/lib/python3.10/site-packages (from timm==0.4.12) (0.15.1+cu118)\n",
      "Requirement already satisfied: jinja2 in /home/srikanth/graspenv/lib/python3.10/site-packages (from torch>=1.4->timm==0.4.12) (3.1.2)\n",
      "Requirement already satisfied: filelock in /home/srikanth/graspenv/lib/python3.10/site-packages (from torch>=1.4->timm==0.4.12) (3.9.0)\n",
      "Requirement already satisfied: networkx in /home/srikanth/graspenv/lib/python3.10/site-packages (from torch>=1.4->timm==0.4.12) (3.0)\n",
      "Requirement already satisfied: sympy in /home/srikanth/graspenv/lib/python3.10/site-packages (from torch>=1.4->timm==0.4.12) (1.12)\n",
      "Requirement already satisfied: typing-extensions in /home/srikanth/graspenv/lib/python3.10/site-packages (from torch>=1.4->timm==0.4.12) (4.9.0)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/srikanth/graspenv/lib/python3.10/site-packages (from torch>=1.4->timm==0.4.12) (2.0.0)\n",
      "Requirement already satisfied: lit in /home/srikanth/graspenv/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.4->timm==0.4.12) (15.0.7)\n",
      "Requirement already satisfied: cmake in /home/srikanth/graspenv/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.4->timm==0.4.12) (3.25.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/srikanth/graspenv/lib/python3.10/site-packages (from torchvision->timm==0.4.12) (9.3.0)\n",
      "Requirement already satisfied: numpy in /home/srikanth/graspenv/lib/python3.10/site-packages (from torchvision->timm==0.4.12) (1.23.5)\n",
      "Requirement already satisfied: requests in /home/srikanth/graspenv/lib/python3.10/site-packages (from torchvision->timm==0.4.12) (2.31.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/srikanth/graspenv/lib/python3.10/site-packages (from jinja2->torch>=1.4->timm==0.4.12) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/srikanth/graspenv/lib/python3.10/site-packages (from requests->torchvision->timm==0.4.12) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/srikanth/graspenv/lib/python3.10/site-packages (from requests->torchvision->timm==0.4.12) (2023.11.17)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/srikanth/graspenv/lib/python3.10/site-packages (from requests->torchvision->timm==0.4.12) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/srikanth/graspenv/lib/python3.10/site-packages (from requests->torchvision->timm==0.4.12) (2.1.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/srikanth/graspenv/lib/python3.10/site-packages (from sympy->torch>=1.4->timm==0.4.12) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install timm==0.4.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17371/3799284888.py:14: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from einops import rearrange\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler,autocast\n",
    "import torch.optim as optim\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder , OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Palmar wrist pronated', 'Pinch', 'Tripod', 'Palmar wrist neutral']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_path = r\"/home/srikanth/Interns/RGB_images\"\n",
    "dataset_path = os.listdir(root_path)\n",
    "dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = []\n",
    "\n",
    "\n",
    "for item in dataset_path:\n",
    "    #print(item)\n",
    "    all_objects = os.listdir(root_path + '/' +item)\n",
    "    for top_object in all_objects:\n",
    "        sub_objects = os.listdir(root_path  + '/' +item + '/' +top_object)\n",
    "        for sub_object in sub_objects:\n",
    "            images = os.listdir(root_path + '/' +item + '/' +top_object + '/' +sub_object)\n",
    "            for image in images:\n",
    "                class_labels.append((item,str(root_path + '/' +item + '/' +top_object + '/' +sub_object +'/' +image)))\n",
    "# class_labels\n",
    "df = pd.DataFrame(data=class_labels, columns=['labels', 'image'])\n",
    "# df\n",
    "y=list(df['labels'].values)\n",
    "# y\n",
    "image=df['image']\n",
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, y= shuffle(image,y, random_state=1)\n",
    "train_x, test_x, train_y, test_y = train_test_split(images, y, test_size=0.3, random_state=415)\n",
    "test_x = test_x.reset_index(drop=True)\n",
    "train_x = train_x.reset_index(drop=True)\n",
    "test_x, val_x, test_y, val_y = train_test_split(test_x,test_y, test_size=0.5, random_state=415)\n",
    "test_x = test_x.reset_index(drop=True)\n",
    "#train_y=list(train_y)\n",
    "train_df=pd.DataFrame({'filepaths':train_x,'labels':train_y})\n",
    "valid_df=pd.DataFrame({'filepaths':val_x,'labels':val_y})\n",
    "test_df=pd.DataFrame({'filepaths':test_x,'labels':test_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=list(train_df['labels'].unique())\n",
    "class_count=len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Palmar wrist pronated': 0, 'Pinch': 1, 'Tripod': 2, 'Palmar wrist neutral': 3}\n",
      "{0: 'Palmar wrist pronated', 1: 'Pinch', 2: 'Tripod', 3: 'Palmar wrist neutral'}\n"
     ]
    }
   ],
   "source": [
    "labels = df['labels'].unique()\n",
    "num_labels = len(labels)\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label\n",
    "\n",
    "print(label2id)\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset():\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((64, 64), antialias=True),\n",
    "        transforms.Normalize( mean= [0.51158103, 0.47950193, 0.46153474],\n",
    "                             std=[0.22355489, 0.22948845, 0.24873442])\n",
    "        ])\n",
    "        self.label_mapping = label2id\n",
    "    # class ImageDataset(Dataset):\n",
    "    # def __init__(self, df, label2id, input_size=224, transform=None):\n",
    "    #     self.df = df\n",
    "    #     self.label_mapping = label2id\n",
    "    #     resize_value = self.calculate_resize_value(input_size)\n",
    "    #     self.transform = transform if transform else transforms.Compose([\n",
    "    #         transforms.Resize((resize_value, resize_value), antialias=True),\n",
    "    #         transforms.CenterCrop(input_size),\n",
    "    #         transforms.ToTensor(),\n",
    "    #         transforms.Normalize(mean=[0.51158103, 0.47950193, 0.46153474],\n",
    "    #                              std=[0.22355489, 0.22948845, 0.24873442])\n",
    "    #     ])\n",
    "\n",
    "    # def calculate_resize_value(self, input_size):\n",
    "    #     return int((256 / 224) * input_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def get_images(self, idx):\n",
    "        return self.transform(Image.open(self.df.iloc[idx]['filepaths']))\n",
    "\n",
    "    def get_labels(self, idx):\n",
    "        label = self.df.iloc[idx]['labels']\n",
    "        return torch.tensor(self.label_mapping[label], dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        train_images = self.get_images(idx)\n",
    "        train_labels = self.get_labels(idx)\n",
    "\n",
    "        return train_images, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset(train_df, transform=transforms)\n",
    "val_dataset = ImageDataset(valid_df, transform=transforms)\n",
    "test_dataset = ImageDataset(test_df, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "\n",
    "from timm.models.layers import DropPath, trunc_normal_\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=partial(nn.LayerNorm, eps=1e-6)):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, patch_size=16, in_chans=3, num_classes=1000, embed_dim=576, depth=12, num_heads=12, mlp_ratio=4.,\n",
    "                 qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.trans_dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
    "\n",
    "        self.trans_patch_conv = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, padding=0)\n",
    "        self.trans_blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=self.trans_dpr[i]\n",
    "            ) for i in range(depth)\n",
    "        ])\n",
    "        self.trans_norm = nn.LayerNorm(embed_dim)\n",
    "        self.trans_cls_head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.weight, 1.)\n",
    "            nn.init.constant_(m.bias, 0.)\n",
    "        elif isinstance(m, nn.GroupNorm):\n",
    "            nn.init.constant_(m.weight, 1.)\n",
    "            nn.init.constant_(m.bias, 0.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = self.trans_patch_conv(x).flatten(2).transpose(1, 2)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "\n",
    "        for block in self.trans_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.trans_norm(x)\n",
    "        tran_cls = self.trans_cls_head(x[:, 0])\n",
    "\n",
    "        return tran_cls\n",
    "\n",
    "# Create a transformer model\n",
    "model = Transformer(patch_size=16, in_chans=3, num_classes=4, embed_dim=576, depth=12, num_heads=9, mlp_ratio=4, qkv_bias=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001,  weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def trainVal(model, criterion, optimizer, num_epochs, min_val_loss, train_loader, val_loader, device):\n",
    "    best_acc = 0.0\n",
    "    min_loss = min_val_loss\n",
    "\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Using tqdm for progress tracking\n",
    "        for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch}', leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                if isinstance(outputs, list):\n",
    "                    loss_list = [criterion(o, labels) / len(outputs) for o in outputs]\n",
    "                    loss = sum(loss_list)\n",
    "                    preds = torch.max(outputs[0] + outputs[1], 1)[1]\n",
    "                else:\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accs.append(epoch_acc)\n",
    "        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluate mode\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                if isinstance(outputs, list):\n",
    "                    loss_list = [criterion(o, labels) / len(outputs) for o in outputs]\n",
    "                    loss = sum(loss_list)\n",
    "                    preds = torch.max(outputs[0] + outputs[1], 1)[1]\n",
    "                else:\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(val_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(val_loader.dataset)\n",
    "\n",
    "        val_losses.append(epoch_loss)\n",
    "        val_accs.append(epoch_acc)\n",
    "        print(f'Val Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        # Update the learning rate\n",
    "        # scheduler.step()  # Uncomment if using a learning rate scheduler\n",
    "\n",
    "        # Save the model if it has the best validation accuracy so far\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            state = {\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'min_loss': epoch_loss\n",
    "            }\n",
    "        torch.save(state, '/home/srikanth/Interns/Rajasree/CSIO-Conformer/weight/simple-cnn.pth')\n",
    "\n",
    "    return train_losses, train_accs, val_losses, val_accs, min_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8757 Acc: 0.6390\n",
      "Val Loss: 0.7241 Acc: 0.7212\n",
      "Epoch 1/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6738 Acc: 0.7396\n",
      "Val Loss: 0.6413 Acc: 0.7557\n",
      "Epoch 2/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5827 Acc: 0.7787\n",
      "Val Loss: 0.5629 Acc: 0.7868\n",
      "Epoch 3/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5210 Acc: 0.8055\n",
      "Val Loss: 0.5203 Acc: 0.8061\n",
      "Epoch 4/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4773 Acc: 0.8230\n",
      "Val Loss: 0.4743 Acc: 0.8255\n",
      "Epoch 5/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4457 Acc: 0.8368\n",
      "Val Loss: 0.4060 Acc: 0.8522\n",
      "Epoch 6/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4205 Acc: 0.8465\n",
      "Val Loss: 0.4503 Acc: 0.8370\n",
      "Epoch 7/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3989 Acc: 0.8560\n",
      "Val Loss: 0.3747 Acc: 0.8669\n",
      "Epoch 8/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8:  67%|██████▋   | 8831/13170 [07:08<03:30, 20.64it/s]"
     ]
    }
   ],
   "source": [
    "# Define the initial minimum validation loss\n",
    "min_val_loss = float('inf')\n",
    "\n",
    "# Call the training function with the appropriate data loaders\n",
    "train_losses, train_accs, val_losses, val_accs, min_loss = trainVal(\n",
    "    model, criterion, optimizer, num_epochs, min_val_loss, train_loader, val_loader, device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the tensors to NumPy arrays\n",
    "\n",
    "train_losses = torch.tensor(train_losses)\n",
    "val_losses = torch.tensor(val_losses)\n",
    "train_accs = torch.tensor(train_accs)\n",
    "val_accs = torch.tensor(val_accs)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accs, label='Training Accuracy')\n",
    "plt.plot(val_accs, label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "model.eval()\n",
    "total_test_loss = 0.0\n",
    "correct_test = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # If the model outputs a list of tensors\n",
    "        if isinstance(outputs, list):\n",
    "            batch_loss = 0.0\n",
    "            for output in outputs:\n",
    "                batch_loss += criterion(output, labels).item()\n",
    "            test_loss = batch_loss / len(outputs)\n",
    "            total_test_loss += test_loss * images.size(0)\n",
    "\n",
    "            # For simplicity, assuming the first output for prediction evaluation\n",
    "            output = outputs[0]\n",
    "        else:\n",
    "            test_loss = criterion(outputs, labels).item()\n",
    "            total_test_loss += test_loss * images.size(0)\n",
    "            output = outputs\n",
    "\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct_test += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        \n",
    "        # Gather predictions and true labels for confusion matrix\n",
    "        all_preds.extend(pred.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "average_test_loss = total_test_loss / len(test_loader.dataset)\n",
    "test_losses.append(average_test_loss)\n",
    "test_accuracies.append(100. * correct_test / len(test_loader.dataset))\n",
    "print(\"Test Accuracy: {:.2f}%\".format(test_accuracies[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "# Classification Report\n",
    "class_report = classification_report(all_labels, all_preds)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graspenv",
   "language": "python",
   "name": "graspenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
