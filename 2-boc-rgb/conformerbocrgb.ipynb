{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /home/srikanth/graspenv/lib/python3.10/site-packages (0.7.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mping\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder , OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = '/home/srikanth/Downloads/conformer_helpers'\n",
    "sys.path.append(os.path.dirname(module_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = '/home/srikanth/Downloads/going_modular'\n",
    "sys.path.append(os.path.dirname(module_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular.helper_functions import set_seeds\n",
    "from going_modular import engine_new\n",
    "from going_modular.helper_functions import plot_loss_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from conformer_helpers import engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = r\"/home/srikanth/Interns/RGB_images\"\n",
    "dataset_path = os.listdir(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = []\n",
    "\n",
    "for item in dataset_path:\n",
    "    #print(item)\n",
    "    all_classes = os.listdir(root_path + '/' +item)\n",
    "    for top_object in all_classes:\n",
    "        sub_objects = os.listdir(root_path  + '/' +item + '/' +top_object)\n",
    "        for sub_object in sub_objects:\n",
    "            class_labels.append((item,str(root_path + '/' +item + '/' +top_object + '/' +sub_object)))\n",
    "\n",
    "df = pd.DataFrame(data=class_labels, columns=['labels', 'image'])\n",
    "y=list(df['labels'].values)\n",
    "image=df['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_dataloader = DataLoader(\n",
    "      df,\n",
    "      batch_size=16,\n",
    "      shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Palmar wrist pronated': 0, 'Pinch': 1, 'Tripod': 2, 'Palmar wrist neutral': 3}\n",
      "{0: 'Palmar wrist pronated', 1: 'Pinch', 2: 'Tripod', 3: 'Palmar wrist neutral'}\n"
     ]
    }
   ],
   "source": [
    "labels = df['labels'].unique()\n",
    "num_labels = len(labels)\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label\n",
    "\n",
    "print(label2id)\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset():\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((64, 64), antialias=True),\n",
    "        transforms.Normalize( mean= [0.51158103, 0.47950193, 0.46153474],\n",
    "                             std=[0.22355489, 0.22948845, 0.24873442])\n",
    "        ])\n",
    "        self.label_mapping = label2id\n",
    "    # class ImageDataset(Dataset):\n",
    "    # def __init__(self, df, label2id, input_size=224, transform=None):\n",
    "    #     self.df = df\n",
    "    #     self.label_mapping = label2id\n",
    "    #     resize_value = self.calculate_resize_value(input_size)\n",
    "    #     self.transform = transform if transform else transforms.Compose([\n",
    "    #         transforms.Resize((resize_value, resize_value), antialias=True),\n",
    "    #         transforms.CenterCrop(input_size),\n",
    "    #         transforms.ToTensor(),\n",
    "    #         transforms.Normalize(mean=[0.51158103, 0.47950193, 0.46153474],\n",
    "    #                              std=[0.22355489, 0.22948845, 0.24873442])\n",
    "    #     ])\n",
    "\n",
    "    # def calculate_resize_value(self, input_size):\n",
    "    #     return int((256 / 224) * input_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def get_images(self, idx):\n",
    "        return self.transform(Image.open(self.df.iloc[idx]['filepaths']))\n",
    "\n",
    "    def get_labels(self, idx):\n",
    "        label = self.df.iloc[idx]['labels']\n",
    "        return torch.tensor(self.label_mapping[label], dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        train_images = self.get_images(idx)\n",
    "        train_labels = self.get_labels(idx)\n",
    "\n",
    "        return train_images, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(train_df,test_df,val_df,batch_size: int):\n",
    "    # Use ImageFolder to create dataset(s)\n",
    "    train_data = ImageDataset(train_df)\n",
    "    test_data = ImageDataset(test_df)\n",
    "    val_data = ImageDataset(val_df)\n",
    "\n",
    "    # Get class names\n",
    "    class_names = list(train_df['labels'].unique())\n",
    "\n",
    "   # Turn images into data loaders\n",
    "    train_dataloader = DataLoader(\n",
    "      train_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=True)\n",
    "    test_dataloader = DataLoader(\n",
    "      test_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=False)\n",
    "    val_dataloader = DataLoader(\n",
    "      val_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=False)\n",
    "\n",
    "    return train_dataloader, test_dataloader, val_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "\n",
    "from timm.models.layers import DropPath, trunc_normal_\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=partial(nn.LayerNorm, eps=1e-6)):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, inplanes, outplanes, stride=1, res_conv=False, act_layer=nn.ReLU, groups=1,\n",
    "                 norm_layer=partial(nn.BatchNorm2d, eps=1e-6), drop_block=None, drop_path=None):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        expansion = 4\n",
    "        med_planes = outplanes // expansion\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inplanes, med_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn1 = norm_layer(med_planes)\n",
    "        self.act1 = act_layer(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(med_planes, med_planes, kernel_size=3, stride=stride, groups=groups, padding=1, bias=False)\n",
    "        self.bn2 = norm_layer(med_planes)\n",
    "        self.act2 = act_layer(inplace=True)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(med_planes, outplanes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn3 = norm_layer(outplanes)\n",
    "        self.act3 = act_layer(inplace=True)\n",
    "\n",
    "        if res_conv:\n",
    "            self.residual_conv = nn.Conv2d(inplanes, outplanes, kernel_size=1, stride=stride, padding=0, bias=False)\n",
    "            self.residual_bn = norm_layer(outplanes)\n",
    "\n",
    "        self.res_conv = res_conv\n",
    "        self.drop_block = drop_block\n",
    "        self.drop_path = drop_path\n",
    "\n",
    "    def zero_init_last_bn(self):\n",
    "        nn.init.zeros_(self.bn3.weight)\n",
    "\n",
    "    def forward(self, x, x_t=None, return_x_2=True):\n",
    "        residual = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        if self.drop_block is not None:\n",
    "            x = self.drop_block(x)\n",
    "        x = self.act1(x)\n",
    "\n",
    "        x = self.conv2(x) if x_t is None else self.conv2(x + x_t)\n",
    "        x = self.bn2(x)\n",
    "        if self.drop_block is not None:\n",
    "            x = self.drop_block(x)\n",
    "        x2 = self.act2(x)\n",
    "\n",
    "        x = self.conv3(x2)\n",
    "        x = self.bn3(x)\n",
    "        if self.drop_block is not None:\n",
    "            x = self.drop_block(x)\n",
    "\n",
    "        if self.drop_path is not None:\n",
    "            x = self.drop_path(x)\n",
    "\n",
    "        if self.res_conv:\n",
    "            residual = self.residual_conv(residual)\n",
    "            residual = self.residual_bn(residual)\n",
    "\n",
    "        x += residual\n",
    "        x = self.act3(x)\n",
    "\n",
    "        if return_x_2:\n",
    "            return x, x2\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class FCUDown(nn.Module):\n",
    "    \"\"\" CNN feature maps -> Transformer patch embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inplanes, outplanes, dw_stride, act_layer=nn.GELU,\n",
    "                 norm_layer=partial(nn.LayerNorm, eps=1e-6)):\n",
    "        super(FCUDown, self).__init__()\n",
    "        self.dw_stride = dw_stride\n",
    "\n",
    "        self.conv_project = nn.Conv2d(inplanes, outplanes, kernel_size=1, stride=1, padding=0)\n",
    "        self.sample_pooling = nn.AvgPool2d(kernel_size=dw_stride, stride=dw_stride)\n",
    "\n",
    "        self.ln = norm_layer(outplanes)\n",
    "        self.act = act_layer()\n",
    "\n",
    "    def forward(self, x, x_t):\n",
    "        x = self.conv_project(x)  # [N, C, H, W]\n",
    "\n",
    "        x = self.sample_pooling(x).flatten(2).transpose(1, 2)\n",
    "        x = self.ln(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        x = torch.cat([x_t[:, 0][:, None, :], x], dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class FCUUp(nn.Module):\n",
    "    \"\"\" Transformer patch embeddings -> CNN feature maps\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inplanes, outplanes, up_stride, act_layer=nn.ReLU,\n",
    "                 norm_layer=partial(nn.BatchNorm2d, eps=1e-6),):\n",
    "        super(FCUUp, self).__init__()\n",
    "\n",
    "        self.up_stride = up_stride\n",
    "        self.conv_project = nn.Conv2d(inplanes, outplanes, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn = norm_layer(outplanes)\n",
    "        self.act = act_layer()\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        B, _, C = x.shape\n",
    "        # [N, 197, 384] -> [N, 196, 384] -> [N, 384, 196] -> [N, 384, 14, 14]\n",
    "        x_r = x[:, 1:].transpose(1, 2).reshape(B, C, H, W)\n",
    "        x_r = self.act(self.bn(self.conv_project(x_r)))\n",
    "\n",
    "        return F.interpolate(x_r, size=(H * self.up_stride, W * self.up_stride))\n",
    "\n",
    "\n",
    "class Med_ConvBlock(nn.Module):\n",
    "    \"\"\" special case for Convblock with down sampling,\n",
    "    \"\"\"\n",
    "    def __init__(self, inplanes, act_layer=nn.ReLU, groups=1, norm_layer=partial(nn.BatchNorm2d, eps=1e-6),\n",
    "                 drop_block=None, drop_path=None):\n",
    "\n",
    "        super(Med_ConvBlock, self).__init__()\n",
    "\n",
    "        expansion = 4\n",
    "        med_planes = inplanes // expansion\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inplanes, med_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn1 = norm_layer(med_planes)\n",
    "        self.act1 = act_layer(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(med_planes, med_planes, kernel_size=3, stride=1, groups=groups, padding=1, bias=False)\n",
    "        self.bn2 = norm_layer(med_planes)\n",
    "        self.act2 = act_layer(inplace=True)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(med_planes, inplanes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn3 = norm_layer(inplanes)\n",
    "        self.act3 = act_layer(inplace=True)\n",
    "\n",
    "        self.drop_block = drop_block\n",
    "        self.drop_path = drop_path\n",
    "\n",
    "    def zero_init_last_bn(self):\n",
    "        nn.init.zeros_(self.bn3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        if self.drop_block is not None:\n",
    "            x = self.drop_block(x)\n",
    "        x = self.act1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        if self.drop_block is not None:\n",
    "            x = self.drop_block(x)\n",
    "        x = self.act2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        if self.drop_block is not None:\n",
    "            x = self.drop_block(x)\n",
    "\n",
    "        if self.drop_path is not None:\n",
    "            x = self.drop_path(x)\n",
    "\n",
    "        x += residual\n",
    "        x = self.act3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvTransBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic module for ConvTransformer, keep feature maps for CNN block and patch embeddings for transformer encoder block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inplanes, outplanes, res_conv, stride, dw_stride, embed_dim, num_heads=12, mlp_ratio=4.,\n",
    "                 qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.,\n",
    "                 last_fusion=False, num_med_block=0, groups=1):\n",
    "\n",
    "        super(ConvTransBlock, self).__init__()\n",
    "        expansion = 4\n",
    "        self.cnn_block = ConvBlock(inplanes=inplanes, outplanes=outplanes, res_conv=res_conv, stride=stride, groups=groups)\n",
    "\n",
    "        if last_fusion:\n",
    "            self.fusion_block = ConvBlock(inplanes=outplanes, outplanes=outplanes, stride=2, res_conv=True, groups=groups)\n",
    "        else:\n",
    "            self.fusion_block = ConvBlock(inplanes=outplanes, outplanes=outplanes, groups=groups)\n",
    "\n",
    "        if num_med_block > 0:\n",
    "            self.med_block = []\n",
    "            for i in range(num_med_block):\n",
    "                self.med_block.append(Med_ConvBlock(inplanes=outplanes, groups=groups))\n",
    "            self.med_block = nn.ModuleList(self.med_block)\n",
    "\n",
    "        self.squeeze_block = FCUDown(inplanes=outplanes // expansion, outplanes=embed_dim, dw_stride=dw_stride)\n",
    "\n",
    "        self.expand_block = FCUUp(inplanes=embed_dim, outplanes=outplanes // expansion, up_stride=dw_stride)\n",
    "\n",
    "        self.trans_block = Block(\n",
    "            dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=drop_path_rate)\n",
    "\n",
    "        self.dw_stride = dw_stride\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_med_block = num_med_block\n",
    "        self.last_fusion = last_fusion\n",
    "\n",
    "    def forward(self, x, x_t):\n",
    "        x, x2 = self.cnn_block(x)\n",
    "\n",
    "        _, _, H, W = x2.shape\n",
    "\n",
    "        x_st = self.squeeze_block(x2, x_t)\n",
    "\n",
    "        x_t = self.trans_block(x_st + x_t)\n",
    "\n",
    "        if self.num_med_block > 0:\n",
    "            for m in self.med_block:\n",
    "                x = m(x)\n",
    "\n",
    "        x_t_r = self.expand_block(x_t, H // self.dw_stride, W // self.dw_stride)\n",
    "        x = self.fusion_block(x, x_t_r, return_x_2=False)\n",
    "\n",
    "        return x, x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conformer(nn.Module):\n",
    "\n",
    "    def __init__(self, patch_size=16, in_chans=3, num_classes=1000, base_channel=64, channel_ratio=4, num_med_block=0,\n",
    "                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.):\n",
    "\n",
    "        # Transformer\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        assert depth % 3 == 0\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.trans_dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "\n",
    "        # Classifier head\n",
    "        self.trans_norm = nn.LayerNorm(embed_dim)\n",
    "        self.trans_cls_head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv_cls_head = nn.Linear(int(256 * channel_ratio), num_classes)\n",
    "\n",
    "        # Stem stage: get the feature maps by conv block (copied form resnet.py)\n",
    "        self.conv1 = nn.Conv2d(in_chans, 64, kernel_size=7, stride=2, padding=3, bias=False)  # 1 / 2 [112, 112]\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.act1 = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  # 1 / 4 [56, 56]\n",
    "\n",
    "        # 1 stage\n",
    "        stage_1_channel = int(base_channel * channel_ratio)\n",
    "        trans_dw_stride = patch_size // 4\n",
    "        self.conv_1 = ConvBlock(inplanes=64, outplanes=stage_1_channel, res_conv=True, stride=1)\n",
    "        self.trans_patch_conv = nn.Conv2d(64, embed_dim, kernel_size=trans_dw_stride, stride=trans_dw_stride, padding=0)\n",
    "        self.trans_1 = Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n",
    "                             qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=self.trans_dpr[0],\n",
    "                             )\n",
    "\n",
    "        # 2~4 stage\n",
    "        init_stage = 2\n",
    "        fin_stage = depth // 3 + 1\n",
    "        for i in range(init_stage, fin_stage):\n",
    "            self.add_module('conv_trans_' + str(i),\n",
    "                    ConvTransBlock(\n",
    "                        stage_1_channel, stage_1_channel, False, 1, dw_stride=trans_dw_stride, embed_dim=embed_dim,\n",
    "                        num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                        drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, drop_path_rate=self.trans_dpr[i-1],\n",
    "                        num_med_block=num_med_block\n",
    "                    )\n",
    "            )\n",
    "\n",
    "\n",
    "        stage_2_channel = int(base_channel * channel_ratio * 2)\n",
    "        # 5~8 stage\n",
    "        init_stage = fin_stage # 5\n",
    "        fin_stage = fin_stage + depth // 3 # 9\n",
    "        for i in range(init_stage, fin_stage):\n",
    "            s = 2 if i == init_stage else 1\n",
    "            in_channel = stage_1_channel if i == init_stage else stage_2_channel\n",
    "            res_conv = True if i == init_stage else False\n",
    "            self.add_module('conv_trans_' + str(i),\n",
    "                    ConvTransBlock(\n",
    "                        in_channel, stage_2_channel, res_conv, s, dw_stride=trans_dw_stride // 2, embed_dim=embed_dim,\n",
    "                        num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                        drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, drop_path_rate=self.trans_dpr[i-1],\n",
    "                        num_med_block=num_med_block\n",
    "                    )\n",
    "            )\n",
    "\n",
    "        stage_3_channel = int(base_channel * channel_ratio * 2 * 2)\n",
    "        # 9~12 stage\n",
    "        init_stage = fin_stage  # 9\n",
    "        fin_stage = fin_stage + depth // 3  # 13\n",
    "        for i in range(init_stage, fin_stage):\n",
    "            s = 2 if i == init_stage else 1\n",
    "            in_channel = stage_2_channel if i == init_stage else stage_3_channel\n",
    "            res_conv = True if i == init_stage else False\n",
    "            last_fusion = True if i == depth else False\n",
    "            self.add_module('conv_trans_' + str(i),\n",
    "                    ConvTransBlock(\n",
    "                        in_channel, stage_3_channel, res_conv, s, dw_stride=trans_dw_stride // 4, embed_dim=embed_dim,\n",
    "                        num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                        drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, drop_path_rate=self.trans_dpr[i-1],\n",
    "                        num_med_block=num_med_block, last_fusion=last_fusion\n",
    "                    )\n",
    "            )\n",
    "        self.fin_stage = fin_stage\n",
    "\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.weight, 1.)\n",
    "            nn.init.constant_(m.bias, 0.)\n",
    "        elif isinstance(m, nn.GroupNorm):\n",
    "            nn.init.constant_(m.weight, 1.)\n",
    "            nn.init.constant_(m.bias, 0.)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'cls_token'}\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        # stem stage [N, 3, 224, 224] -> [N, 64, 56, 56]\n",
    "        x_base = self.maxpool(self.act1(self.bn1(self.conv1(x))))\n",
    "\n",
    "        # 1 stage\n",
    "        x = self.conv_1(x_base, return_x_2=False)\n",
    "\n",
    "        x_t = self.trans_patch_conv(x_base).flatten(2).transpose(1, 2)\n",
    "        x_t = torch.cat([cls_tokens, x_t], dim=1)\n",
    "        x_t = self.trans_1(x_t)\n",
    "        \n",
    "        # 2 ~ final \n",
    "        for i in range(2, self.fin_stage):\n",
    "            x, x_t = eval('self.conv_trans_' + str(i))(x, x_t)\n",
    "\n",
    "        # conv classification\n",
    "        x_p = self.pooling(x).flatten(1)\n",
    "        conv_cls = self.conv_cls_head(x_p)\n",
    "\n",
    "        # trans classification\n",
    "        x_t = self.trans_norm(x_t)\n",
    "        tran_cls = self.trans_cls_head(x_t[:, 0])\n",
    "\n",
    "        return [conv_cls, tran_cls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=0.0001\n",
    "EPOCHS=1\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# import torch\n",
    "\n",
    "def trainVal(model, criterion, optimizer, num_epochs, min_val_loss, train_loader, val_loader, device):\n",
    "    best_acc = 0.0\n",
    "    min_loss = min_val_loss\n",
    "\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Using tqdm for progress tracking\n",
    "        for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch}', leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                if isinstance(outputs, list):\n",
    "                    loss_list = [criterion(o, labels) / len(outputs) for o in outputs]\n",
    "                    loss = sum(loss_list)\n",
    "                    preds = torch.max(outputs[0] + outputs[1], 1)[1]\n",
    "                else:\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accs.append(epoch_acc)\n",
    "        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluate mode\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                if isinstance(outputs, list):\n",
    "                    loss_list = [criterion(o, labels) / len(outputs) for o in outputs]\n",
    "                    loss = sum(loss_list)\n",
    "                    preds = torch.max(outputs[0] + outputs[1], 1)[1]\n",
    "                else:\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(val_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(val_loader.dataset)\n",
    "\n",
    "        val_losses.append(epoch_loss)\n",
    "        val_accs.append(epoch_acc)\n",
    "        print(f'Val Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        # Update the learning rate\n",
    "        # scheduler.step()  # Uncomment if using a learning rate scheduler\n",
    "\n",
    "        # Save the model if it has the best validation accuracy so far\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            state = {\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'min_loss': epoch_loss\n",
    "            }\n",
    "        torch.save(state, '/home/srikanth/Interns/Rajasree/CSIO-Conformer/weight/conformer-boc-rgbd.pth')\n",
    "\n",
    "    return train_losses, train_accs, val_losses, val_accs, min_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2926 Acc: 0.9171\n",
      "Val Loss: 1.8891 Acc: 0.6578\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "[[4915  306  698 1379]\n",
      " [  40 6910 1105  354]\n",
      " [1029 1028 4863  411]\n",
      " [ 167  163  108 5415]]\n",
      "Precision: 0.7683676621063257\n",
      "Recall: 0.765047938804472\n",
      "F1 score: 0.7623914224231084\n",
      "Epoch 0/0\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 81\u001b[0m\n\u001b[1;32m     75\u001b[0m min_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Set the seeds\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m#set_seeds()\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Train the model and save the training results to a dictionary\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m train_losses, train_accs, val_losses, val_accs, min_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainVal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_val_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Convert the tensors to NumPy arrays\u001b[39;00m\n\u001b[1;32m     86\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(train_losses)\n",
      "Cell \u001b[0;32mIn[74], line 31\u001b[0m, in \u001b[0;36mtrainVal\u001b[0;34m(model, criterion, optimizer, num_epochs, min_val_loss, train_loader, val_loader, device)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# track history if only in train\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     33\u001b[0m         loss_list \u001b[38;5;241m=\u001b[39m [criterion(o, labels) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(outputs) \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/graspenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[72], line 130\u001b[0m, in \u001b[0;36mConformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# 2 ~ final \u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfin_stage):\n\u001b[0;32m--> 130\u001b[0m     x, x_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mself.conv_trans_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# conv classification\u001b[39;00m\n\u001b[1;32m    133\u001b[0m x_p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling(x)\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/graspenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[71], line 288\u001b[0m, in \u001b[0;36mConvTransBlock.forward\u001b[0;34m(self, x, x_t)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, x_t):\n\u001b[0;32m--> 288\u001b[0m     x, x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnn_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m     _, _, H, W \u001b[38;5;241m=\u001b[39m x2\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    292\u001b[0m     x_st \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msqueeze_block(x2, x_t)\n",
      "File \u001b[0;32m~/graspenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[71], line 117\u001b[0m, in \u001b[0;36mConvBlock.forward\u001b[0;34m(self, x, x_t, return_x_2)\u001b[0m\n\u001b[1;32m    114\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact1(x)\n\u001b[1;32m    116\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x) \u001b[38;5;28;01mif\u001b[39;00m x_t \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x \u001b[38;5;241m+\u001b[39m x_t)\n\u001b[0;32m--> 117\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_block \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_block(x)\n",
      "File \u001b[0;32m~/graspenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/graspenv/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/graspenv/lib/python3.10/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2451\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2452\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "features = [item[1] for item in class_labels]\n",
    "labels = [item[0] for item in class_labels]\n",
    "num_folds = 5\n",
    "kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "kfold.split(features, labels)\n",
    "\n",
    "average_validation_accuracy = []\n",
    "for train_indices, test_indices in kfold.split(features, labels):\n",
    "    train_data = [(labels[i], features[i]) for i in train_indices]\n",
    "    test_datas = [(labels[i], features[i]) for i in test_indices]\n",
    "    #print(train_indices)\n",
    "    num_fold = 2\n",
    "    test_features = [item[1] for item in test_datas]\n",
    "    test_labels = [item[0] for item in test_datas]\n",
    "    kfold_test = StratifiedKFold(n_splits=num_fold, shuffle=True, random_state=42)\n",
    "    for val_indice, test_indice in kfold_test.split(test_features, test_labels):\n",
    "        val_data = [(test_labels[i], test_features[i]) for i in val_indice]\n",
    "        test_data = [(test_labels[i], test_features[i]) for i in test_indice]\n",
    "\n",
    "    # For getting the value address of train image\n",
    "    new_train_value=[]\n",
    "    for room in train_data:\n",
    "        photo = os.listdir(room[1])\n",
    "        for image in photo:\n",
    "            new_train_value.append((room[0],str(room[1] +'/' +image)))\n",
    "\n",
    "    #Creating the Dataframe for Train\n",
    "    train_df= pd.DataFrame(data=new_train_value, columns=['labels', 'filepaths'])\n",
    "\n",
    "    # For getting the value address of test image\n",
    "    new_test_value=[]\n",
    "    for room in test_data:\n",
    "        photo = os.listdir(room[1])\n",
    "        for image in photo:\n",
    "            new_test_value.append((room[0],str(room[1] +'/' +image)))\n",
    "\n",
    "    #Creating the Dataframe for Testing\n",
    "    test_df= pd.DataFrame(data=new_test_value, columns=['labels', 'filepaths'])\n",
    "\n",
    "    # For getting the value address of Validation image\n",
    "    new_val_value=[]\n",
    "    for room in val_data:\n",
    "        photo = os.listdir(room[1])\n",
    "        for image in photo:\n",
    "            new_val_value.append((room[0],str(room[1] +'/' +image)))\n",
    "\n",
    "    #Creating the Dataframe for Validation\n",
    "    val_df= pd.DataFrame(data=new_val_value, columns=['labels', 'filepaths'])\n",
    "\n",
    "\n",
    "#############################################################################################################################################\n",
    "    # Create data loaders\n",
    "    train_gen, test_gen, valid_gen, class_names = create_dataloaders(\n",
    "        train_df=train_df,\n",
    "        test_df=test_df,\n",
    "        val_df=val_df,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    train_gen, test_gen, valid_gen, class_names\n",
    "    classes=labels\n",
    "    class_count=len(classes)\n",
    "\n",
    "\n",
    "#############################################################################################################################################\n",
    "\n",
    "    # Setup the optimizer to optimize our ViT model parameters using hyperparameters from the ViT paper\n",
    "    model = Conformer(patch_size=16, channel_ratio=6, embed_dim=576, depth=12,\n",
    "                      num_heads=9, mlp_ratio=4, qkv_bias=True)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=LR)\n",
    "\n",
    "    # Setup the loss function for multi-class classification\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    min_val_loss = float('inf')\n",
    "\n",
    "    # Set the seeds\n",
    "    #set_seeds()\n",
    "    # Train the model and save the training results to a dictionary\n",
    "    \n",
    "    train_losses, train_accs, val_losses, val_accs, min_loss = trainVal(model, loss_fn, optimizer, EPOCHS, min_val_loss, train_gen, valid_gen, device)\n",
    "\n",
    "    \n",
    "    # Convert the tensors to NumPy arrays\n",
    "\n",
    "    train_losses = torch.tensor(train_losses)\n",
    "    val_losses = torch.tensor(val_losses)\n",
    "    train_accs = torch.tensor(train_accs)\n",
    "    val_accs = torch.tensor(val_accs)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label='Training Accuracy')\n",
    "    plt.plot(val_accs, label='Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    #############################################################################################################################################\n",
    "    # EVALUATION\n",
    "    # model.eval()\n",
    "    # y_true=[]\n",
    "    # y_pred=[]\n",
    "    # for _,(image, label) in enumerate(test_gen):\n",
    "    #     y_true.extend(label.cpu().numpy())\n",
    "    #     prediction =torch.argmax(torch.softmax(model(image.to(device)),dim=1),dim=1)\n",
    "    #     prediction=prediction.cpu().numpy()\n",
    "    #     y_pred.extend(prediction)\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for _, (image, label) in enumerate(test_gen):\n",
    "        y_true.extend(label.cpu().numpy())\n",
    "        outputs = model(image.to(device))\n",
    "\n",
    "        # Handle the case where the output is a list of tensors\n",
    "        if isinstance(outputs, list):\n",
    "            batch_predictions = []\n",
    "            for output in outputs:\n",
    "                batch_predictions.append(torch.argmax(torch.softmax(output, dim=1), dim=1))\n",
    "            # For simplicity, use the first output for prediction evaluation\n",
    "            prediction = batch_predictions[0]\n",
    "        else:\n",
    "            prediction = torch.argmax(torch.softmax(outputs, dim=1), dim=1)\n",
    "\n",
    "        prediction = prediction.cpu().numpy()\n",
    "        y_pred.extend(prediction)\n",
    "\n",
    "\n",
    "    #confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(f\"Confusion matrix: \\n{cm}\")\n",
    "\n",
    "    #Metrics\n",
    "\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    model.eval()\n",
    "\n",
    "    Sum = 0\n",
    "    for inputs, labels in test_gen:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        output = model(inputs).to(device).float()\n",
    "\n",
    "        _,prediction = torch.max(output,1)\n",
    "\n",
    "        pred_label = labels[prediction]\n",
    "        pred_label = pred_label.detach().cpu().numpy()\n",
    "        main_label = labels.detach().cpu().numpy()\n",
    "        bool_list  = list(map(lambda x, y: x == y, pred_label, main_label))\n",
    "        Sum += sum(np.array(bool_list)*1)\n",
    "\n",
    "    print('Prediction: ', (Sum/len(test_gen.dataset)*100,'%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "model.eval()\n",
    "total_test_loss = 0.0\n",
    "correct_test = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # If the model outputs a list of tensors\n",
    "        if isinstance(outputs, list):\n",
    "            batch_loss = 0.0\n",
    "            for output in outputs:\n",
    "                batch_loss += criterion(output, labels).item()\n",
    "            test_loss = batch_loss / len(outputs)\n",
    "            total_test_loss += test_loss * images.size(0)\n",
    "\n",
    "            # For simplicity, assuming the first output for prediction evaluation\n",
    "            output = outputs[0]\n",
    "        else:\n",
    "            test_loss = criterion(outputs, labels).item()\n",
    "            total_test_loss += test_loss * images.size(0)\n",
    "            output = outputs\n",
    "\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct_test += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        \n",
    "        # Gather predictions and true labels for confusion matrix\n",
    "        all_preds.extend(pred.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "average_test_loss = total_test_loss / len(test_loader.dataset)\n",
    "test_losses.append(average_test_loss)\n",
    "test_accuracies.append(100. * correct_test / len(test_loader.dataset))\n",
    "print(\"Test Accuracy: {:.2f}%\".format(test_accuracies[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "# Classification Report\n",
    "class_report = classification_report(all_labels, all_preds)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graspenv",
   "language": "python",
   "name": "graspenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
