{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from einops import rearrange\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler,autocast\n",
    "import torch.optim as optim\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder , OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Palmar wrist pronated', 'Pinch', 'Tripod', 'Palmar wrist neutral']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_path = r\"/home/srikanth/Dataset/RGB_images\"\n",
    "dataset_path = os.listdir(root_path)\n",
    "dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = []\n",
    "\n",
    "\n",
    "for item in dataset_path:\n",
    "    #print(item)\n",
    "    all_objects = os.listdir(root_path + '/' +item)\n",
    "    for top_object in all_objects:\n",
    "        sub_objects = os.listdir(root_path  + '/' +item + '/' +top_object)\n",
    "        for sub_object in sub_objects:\n",
    "            images = os.listdir(root_path + '/' +item + '/' +top_object + '/' +sub_object)\n",
    "            for image in images:\n",
    "                class_labels.append((item,str(root_path + '/' +item + '/' +top_object + '/' +sub_object +'/' +image)))\n",
    "# class_labels\n",
    "df = pd.DataFrame(data=class_labels, columns=['labels', 'image'])\n",
    "# df\n",
    "y=list(df['labels'].values)\n",
    "# y\n",
    "image=df['image']\n",
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, y= shuffle(image,y, random_state=1)\n",
    "train_x, test_x, train_y, test_y = train_test_split(images, y, test_size=0.2, random_state=415)\n",
    "test_x = test_x.reset_index(drop=True)\n",
    "train_x = train_x.reset_index(drop=True)\n",
    "test_x, val_x, test_y, val_y = train_test_split(test_x,test_y, test_size=0.5, random_state=415)\n",
    "test_x = test_x.reset_index(drop=True)\n",
    "#train_y=list(train_y)\n",
    "train_df=pd.DataFrame({'filepaths':train_x,'labels':train_y})\n",
    "valid_df=pd.DataFrame({'filepaths':val_x,'labels':val_y})\n",
    "test_df=pd.DataFrame({'filepaths':test_x,'labels':test_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=list(train_df['labels'].unique())\n",
    "class_count=len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Palmar wrist pronated': 0, 'Pinch': 1, 'Tripod': 2, 'Palmar wrist neutral': 3}\n",
      "{0: 'Palmar wrist pronated', 1: 'Pinch', 2: 'Tripod', 3: 'Palmar wrist neutral'}\n"
     ]
    }
   ],
   "source": [
    "labels = df['labels'].unique()\n",
    "num_labels = len(labels)\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label\n",
    "\n",
    "print(label2id)\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset():\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((64, 64), antialias=True),\n",
    "        transforms.Normalize( mean= [0.51158103, 0.47950193, 0.46153474],\n",
    "                             std=[0.22355489, 0.22948845, 0.24873442])\n",
    "        ])\n",
    "        self.label_mapping = label2id\n",
    "    # class ImageDataset(Dataset):\n",
    "    # def __init__(self, df, label2id, input_size=224, transform=None):\n",
    "    #     self.df = df\n",
    "    #     self.label_mapping = label2id\n",
    "    #     resize_value = self.calculate_resize_value(input_size)\n",
    "    #     self.transform = transform if transform else transforms.Compose([\n",
    "    #         transforms.Resize((resize_value, resize_value), antialias=True),\n",
    "    #         transforms.CenterCrop(input_size),\n",
    "    #         transforms.ToTensor(),\n",
    "    #         transforms.Normalize(mean=[0.51158103, 0.47950193, 0.46153474],\n",
    "    #                              std=[0.22355489, 0.22948845, 0.24873442])\n",
    "    #     ])\n",
    "\n",
    "    # def calculate_resize_value(self, input_size):\n",
    "    #     return int((256 / 224) * input_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def get_images(self, idx):\n",
    "        return self.transform(Image.open(self.df.iloc[idx]['filepaths']))\n",
    "\n",
    "    def get_labels(self, idx):\n",
    "        label = self.df.iloc[idx]['labels']\n",
    "        return torch.tensor(self.label_mapping[label], dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        train_images = self.get_images(idx)\n",
    "        train_labels = self.get_labels(idx)\n",
    "\n",
    "        return train_images, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset(train_df, transform=transforms)\n",
    "val_dataset = ImageDataset(valid_df, transform=transforms)\n",
    "test_dataset = ImageDataset(test_df, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "\n",
    "from timm.models.layers import DropPath, trunc_normal_\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=partial(nn.LayerNorm, eps=1e-6)):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, inplanes, outplanes, stride=1, res_conv=False, act_layer=nn.ReLU, groups=1,\n",
    "                 norm_layer=partial(nn.BatchNorm2d, eps=1e-6), drop_block=None, drop_path=None):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        expansion = 4\n",
    "        med_planes = outplanes // expansion\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inplanes, med_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn1 = norm_layer(med_planes)\n",
    "        self.act1 = act_layer(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(med_planes, med_planes, kernel_size=3, stride=stride, groups=groups, padding=1, bias=False)\n",
    "        self.bn2 = norm_layer(med_planes)\n",
    "        self.act2 = act_layer(inplace=True)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(med_planes, outplanes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn3 = norm_layer(outplanes)\n",
    "        self.act3 = act_layer(inplace=True)\n",
    "\n",
    "        if res_conv:\n",
    "            self.residual_conv = nn.Conv2d(inplanes, outplanes, kernel_size=1, stride=stride, padding=0, bias=False)\n",
    "            self.residual_bn = norm_layer(outplanes)\n",
    "\n",
    "        self.res_conv = res_conv\n",
    "        self.drop_block = drop_block\n",
    "        self.drop_path = drop_path\n",
    "\n",
    "    def zero_init_last_bn(self):\n",
    "        nn.init.zeros_(self.bn3.weight)\n",
    "\n",
    "    def forward(self, x, x_t=None, return_x_2=True):\n",
    "        residual = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        if self.drop_block is not None:\n",
    "            x = self.drop_block(x)\n",
    "        x = self.act1(x)\n",
    "\n",
    "        x = self.conv2(x) if x_t is None else self.conv2(x + x_t)\n",
    "        x = self.bn2(x)\n",
    "        if self.drop_block is not None:\n",
    "            x = self.drop_block(x)\n",
    "        x2 = self.act2(x)\n",
    "\n",
    "        x = self.conv3(x2)\n",
    "        x = self.bn3(x)\n",
    "        if self.drop_block is not None:\n",
    "            x = self.drop_block(x)\n",
    "\n",
    "        if self.drop_path is not None:\n",
    "            x = self.drop_path(x)\n",
    "\n",
    "        if self.res_conv:\n",
    "            residual = self.residual_conv(residual)\n",
    "            residual = self.residual_bn(residual)\n",
    "\n",
    "        x += residual\n",
    "        x = self.act3(x)\n",
    "\n",
    "        if return_x_2:\n",
    "            return x, x2\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class FCUDown(nn.Module):\n",
    "    \"\"\" CNN feature maps -> Transformer patch embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inplanes, outplanes, dw_stride, act_layer=nn.GELU,\n",
    "                 norm_layer=partial(nn.LayerNorm, eps=1e-6)):\n",
    "        super(FCUDown, self).__init__()\n",
    "        self.dw_stride = dw_stride\n",
    "\n",
    "        self.conv_project = nn.Conv2d(inplanes, outplanes, kernel_size=1, stride=1, padding=0)\n",
    "        self.sample_pooling = nn.AvgPool2d(kernel_size=dw_stride, stride=dw_stride)\n",
    "\n",
    "        self.ln = norm_layer(outplanes)\n",
    "        self.act = act_layer()\n",
    "\n",
    "    def forward(self, x, x_t):\n",
    "        x = self.conv_project(x)  # [N, C, H, W]\n",
    "\n",
    "        x = self.sample_pooling(x).flatten(2).transpose(1, 2)\n",
    "        x = self.ln(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        x = torch.cat([x_t[:, 0][:, None, :], x], dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class FCUUp(nn.Module):\n",
    "    \"\"\" Transformer patch embeddings -> CNN feature maps\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inplanes, outplanes, up_stride, act_layer=nn.ReLU,\n",
    "                 norm_layer=partial(nn.BatchNorm2d, eps=1e-6),):\n",
    "        super(FCUUp, self).__init__()\n",
    "\n",
    "        self.up_stride = up_stride\n",
    "        self.conv_project = nn.Conv2d(inplanes, outplanes, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn = norm_layer(outplanes)\n",
    "        self.act = act_layer()\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        B, _, C = x.shape\n",
    "        # [N, 197, 384] -> [N, 196, 384] -> [N, 384, 196] -> [N, 384, 14, 14]\n",
    "        x_r = x[:, 1:].transpose(1, 2).reshape(B, C, H, W)\n",
    "        x_r = self.act(self.bn(self.conv_project(x_r)))\n",
    "\n",
    "        return F.interpolate(x_r, size=(H * self.up_stride, W * self.up_stride))\n",
    "\n",
    "\n",
    "class Med_ConvBlock(nn.Module):\n",
    "    \"\"\" special case for Convblock with down sampling,\n",
    "    \"\"\"\n",
    "    def __init__(self, inplanes, act_layer=nn.ReLU, groups=1, norm_layer=partial(nn.BatchNorm2d, eps=1e-6),\n",
    "                 drop_block=None, drop_path=None):\n",
    "\n",
    "        super(Med_ConvBlock, self).__init__()\n",
    "\n",
    "        expansion = 4\n",
    "        med_planes = inplanes // expansion\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inplanes, med_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn1 = norm_layer(med_planes)\n",
    "        self.act1 = act_layer(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(med_planes, med_planes, kernel_size=3, stride=1, groups=groups, padding=1, bias=False)\n",
    "        self.bn2 = norm_layer(med_planes)\n",
    "        self.act2 = act_layer(inplace=True)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(med_planes, inplanes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn3 = norm_layer(inplanes)\n",
    "        self.act3 = act_layer(inplace=True)\n",
    "\n",
    "        self.drop_block = drop_block\n",
    "        self.drop_path = drop_path\n",
    "\n",
    "    def zero_init_last_bn(self):\n",
    "        nn.init.zeros_(self.bn3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        if self.drop_block is not None:\n",
    "            x = self.drop_block(x)\n",
    "        x = self.act1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        if self.drop_block is not None:\n",
    "            x = self.drop_block(x)\n",
    "        x = self.act2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        if self.drop_block is not None:\n",
    "            x = self.drop_block(x)\n",
    "\n",
    "        if self.drop_path is not None:\n",
    "            x = self.drop_path(x)\n",
    "\n",
    "        x += residual\n",
    "        x = self.act3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvTransBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic module for ConvTransformer, keep feature maps for CNN block and patch embeddings for transformer encoder block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inplanes, outplanes, res_conv, stride, dw_stride, embed_dim, num_heads=12, mlp_ratio=4.,\n",
    "                 qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.,\n",
    "                 last_fusion=False, num_med_block=0, groups=1):\n",
    "\n",
    "        super(ConvTransBlock, self).__init__()\n",
    "        expansion = 4\n",
    "        self.cnn_block = ConvBlock(inplanes=inplanes, outplanes=outplanes, res_conv=res_conv, stride=stride, groups=groups)\n",
    "\n",
    "        if last_fusion:\n",
    "            self.fusion_block = ConvBlock(inplanes=outplanes, outplanes=outplanes, stride=2, res_conv=True, groups=groups)\n",
    "        else:\n",
    "            self.fusion_block = ConvBlock(inplanes=outplanes, outplanes=outplanes, groups=groups)\n",
    "\n",
    "        if num_med_block > 0:\n",
    "            self.med_block = []\n",
    "            for i in range(num_med_block):\n",
    "                self.med_block.append(Med_ConvBlock(inplanes=outplanes, groups=groups))\n",
    "            self.med_block = nn.ModuleList(self.med_block)\n",
    "\n",
    "        self.squeeze_block = FCUDown(inplanes=outplanes // expansion, outplanes=embed_dim, dw_stride=dw_stride)\n",
    "\n",
    "        self.expand_block = FCUUp(inplanes=embed_dim, outplanes=outplanes // expansion, up_stride=dw_stride)\n",
    "\n",
    "        self.trans_block = Block(\n",
    "            dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=drop_path_rate)\n",
    "\n",
    "        self.dw_stride = dw_stride\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_med_block = num_med_block\n",
    "        self.last_fusion = last_fusion\n",
    "\n",
    "    def forward(self, x, x_t):\n",
    "        x, x2 = self.cnn_block(x)\n",
    "\n",
    "        _, _, H, W = x2.shape\n",
    "\n",
    "        x_st = self.squeeze_block(x2, x_t)\n",
    "\n",
    "        x_t = self.trans_block(x_st + x_t)\n",
    "\n",
    "        if self.num_med_block > 0:\n",
    "            for m in self.med_block:\n",
    "                x = m(x)\n",
    "\n",
    "        x_t_r = self.expand_block(x_t, H // self.dw_stride, W // self.dw_stride)\n",
    "        x = self.fusion_block(x, x_t_r, return_x_2=False)\n",
    "\n",
    "        return x, x_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONFORMER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conformer(nn.Module):\n",
    "\n",
    "    def __init__(self, patch_size=16, in_chans=3, num_classes=1000, base_channel=64, channel_ratio=4, num_med_block=0,\n",
    "                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.):\n",
    "\n",
    "        # Transformer\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        assert depth % 3 == 0\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.trans_dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "\n",
    "        # Classifier head\n",
    "        self.trans_norm = nn.LayerNorm(embed_dim)\n",
    "        self.trans_cls_head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv_cls_head = nn.Linear(int(256 * channel_ratio), num_classes)\n",
    "\n",
    "        # Stem stage: get the feature maps by conv block (copied form resnet.py)\n",
    "        self.conv1 = nn.Conv2d(in_chans, 64, kernel_size=7, stride=2, padding=3, bias=False)  # 1 / 2 [112, 112]\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.act1 = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  # 1 / 4 [56, 56]\n",
    "\n",
    "        # 1 stage\n",
    "        stage_1_channel = int(base_channel * channel_ratio)\n",
    "        trans_dw_stride = patch_size // 4\n",
    "        self.conv_1 = ConvBlock(inplanes=64, outplanes=stage_1_channel, res_conv=True, stride=1)\n",
    "        self.trans_patch_conv = nn.Conv2d(64, embed_dim, kernel_size=trans_dw_stride, stride=trans_dw_stride, padding=0)\n",
    "        self.trans_1 = Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n",
    "                             qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=self.trans_dpr[0],\n",
    "                             )\n",
    "\n",
    "        # 2~4 stage\n",
    "        init_stage = 2\n",
    "        fin_stage = depth // 3 + 1\n",
    "        for i in range(init_stage, fin_stage):\n",
    "            self.add_module('conv_trans_' + str(i),\n",
    "                    ConvTransBlock(\n",
    "                        stage_1_channel, stage_1_channel, False, 1, dw_stride=trans_dw_stride, embed_dim=embed_dim,\n",
    "                        num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                        drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, drop_path_rate=self.trans_dpr[i-1],\n",
    "                        num_med_block=num_med_block\n",
    "                    )\n",
    "            )\n",
    "\n",
    "\n",
    "        stage_2_channel = int(base_channel * channel_ratio * 2)\n",
    "        # 5~8 stage\n",
    "        init_stage = fin_stage # 5\n",
    "        fin_stage = fin_stage + depth // 3 # 9\n",
    "        for i in range(init_stage, fin_stage):\n",
    "            s = 2 if i == init_stage else 1\n",
    "            in_channel = stage_1_channel if i == init_stage else stage_2_channel\n",
    "            res_conv = True if i == init_stage else False\n",
    "            self.add_module('conv_trans_' + str(i),\n",
    "                    ConvTransBlock(\n",
    "                        in_channel, stage_2_channel, res_conv, s, dw_stride=trans_dw_stride // 2, embed_dim=embed_dim,\n",
    "                        num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                        drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, drop_path_rate=self.trans_dpr[i-1],\n",
    "                        num_med_block=num_med_block\n",
    "                    )\n",
    "            )\n",
    "\n",
    "        stage_3_channel = int(base_channel * channel_ratio * 2 * 2)\n",
    "        # 9~12 stage\n",
    "        init_stage = fin_stage  # 9\n",
    "        fin_stage = fin_stage + depth // 3  # 13\n",
    "        for i in range(init_stage, fin_stage):\n",
    "            s = 2 if i == init_stage else 1\n",
    "            in_channel = stage_2_channel if i == init_stage else stage_3_channel\n",
    "            res_conv = True if i == init_stage else False\n",
    "            last_fusion = True if i == depth else False\n",
    "            self.add_module('conv_trans_' + str(i),\n",
    "                    ConvTransBlock(\n",
    "                        in_channel, stage_3_channel, res_conv, s, dw_stride=trans_dw_stride // 4, embed_dim=embed_dim,\n",
    "                        num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                        drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, drop_path_rate=self.trans_dpr[i-1],\n",
    "                        num_med_block=num_med_block, last_fusion=last_fusion\n",
    "                    )\n",
    "            )\n",
    "        self.fin_stage = fin_stage\n",
    "\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.weight, 1.)\n",
    "            nn.init.constant_(m.bias, 0.)\n",
    "        elif isinstance(m, nn.GroupNorm):\n",
    "            nn.init.constant_(m.weight, 1.)\n",
    "            nn.init.constant_(m.bias, 0.)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'cls_token'}\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        # stem stage [N, 3, 224, 224] -> [N, 64, 56, 56]\n",
    "        x_base = self.maxpool(self.act1(self.bn1(self.conv1(x))))\n",
    "\n",
    "        # 1 stage\n",
    "        x = self.conv_1(x_base, return_x_2=False)\n",
    "\n",
    "        x_t = self.trans_patch_conv(x_base).flatten(2).transpose(1, 2)\n",
    "        x_t = torch.cat([cls_tokens, x_t], dim=1)\n",
    "        x_t = self.trans_1(x_t)\n",
    "        \n",
    "        # 2 ~ final \n",
    "        for i in range(2, self.fin_stage):\n",
    "            x, x_t = eval('self.conv_trans_' + str(i))(x, x_t)\n",
    "\n",
    "        # conv classification\n",
    "        x_p = self.pooling(x).flatten(1)\n",
    "        conv_cls = self.conv_cls_head(x_p)\n",
    "\n",
    "        # trans classification\n",
    "        x_t = self.trans_norm(x_t)\n",
    "        tran_cls = self.trans_cls_head(x_t[:, 0])\n",
    "\n",
    "        return [conv_cls, tran_cls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conformer(patch_size=16, channel_ratio=6, embed_dim=576, depth=12,\n",
    "                      num_heads=9, mlp_ratio=4, qkv_bias=True)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001,  weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conformer(\n",
       "  (trans_norm): LayerNorm((576,), eps=1e-05, elementwise_affine=True)\n",
       "  (trans_cls_head): Linear(in_features=576, out_features=1000, bias=True)\n",
       "  (pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (conv_cls_head): Linear(in_features=1536, out_features=1000, bias=True)\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (conv_1): ConvBlock(\n",
       "    (conv1): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(96, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act1): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(96, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act2): ReLU(inplace=True)\n",
       "    (conv3): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn3): BatchNorm2d(384, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act3): ReLU(inplace=True)\n",
       "    (residual_conv): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (residual_bn): BatchNorm2d(384, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (trans_patch_conv): Conv2d(64, 576, kernel_size=(4, 4), stride=(4, 4))\n",
       "  (trans_1): Block(\n",
       "    (norm1): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=576, out_features=1728, bias=True)\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=576, out_features=576, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (drop_path): Identity()\n",
       "    (norm2): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=576, out_features=2304, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (fc2): Linear(in_features=2304, out_features=576, bias=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (conv_trans_2): ConvTransBlock(\n",
       "    (cnn_block): ConvBlock(\n",
       "      (conv1): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(96, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(96, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(384, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (fusion_block): ConvBlock(\n",
       "      (conv1): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(96, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(96, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(384, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (squeeze_block): FCUDown(\n",
       "      (conv_project): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sample_pooling): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "      (ln): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (act): GELU(approximate='none')\n",
       "    )\n",
       "    (expand_block): FCUUp(\n",
       "      (conv_project): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(96, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (trans_block): Block(\n",
       "      (norm1): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=576, out_features=1728, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=576, out_features=576, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=576, out_features=2304, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=2304, out_features=576, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_trans_3): ConvTransBlock(\n",
       "    (cnn_block): ConvBlock(\n",
       "      (conv1): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(96, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(96, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(384, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (fusion_block): ConvBlock(\n",
       "      (conv1): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(96, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(96, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(384, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (squeeze_block): FCUDown(\n",
       "      (conv_project): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sample_pooling): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "      (ln): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (act): GELU(approximate='none')\n",
       "    )\n",
       "    (expand_block): FCUUp(\n",
       "      (conv_project): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(96, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (trans_block): Block(\n",
       "      (norm1): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=576, out_features=1728, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=576, out_features=576, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=576, out_features=2304, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=2304, out_features=576, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_trans_4): ConvTransBlock(\n",
       "    (cnn_block): ConvBlock(\n",
       "      (conv1): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(96, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(96, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(384, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (fusion_block): ConvBlock(\n",
       "      (conv1): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(96, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(96, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(384, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (squeeze_block): FCUDown(\n",
       "      (conv_project): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sample_pooling): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "      (ln): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (act): GELU(approximate='none')\n",
       "    )\n",
       "    (expand_block): FCUUp(\n",
       "      (conv_project): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(96, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (trans_block): Block(\n",
       "      (norm1): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=576, out_features=1728, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=576, out_features=576, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=576, out_features=2304, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=2304, out_features=576, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_trans_5): ConvTransBlock(\n",
       "    (cnn_block): ConvBlock(\n",
       "      (conv1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(192, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(192, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(768, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "      (residual_conv): Conv2d(384, 768, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (residual_bn): BatchNorm2d(768, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (fusion_block): ConvBlock(\n",
       "      (conv1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(192, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(192, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(768, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (squeeze_block): FCUDown(\n",
       "      (conv_project): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sample_pooling): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      (ln): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (act): GELU(approximate='none')\n",
       "    )\n",
       "    (expand_block): FCUUp(\n",
       "      (conv_project): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(192, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (trans_block): Block(\n",
       "      (norm1): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=576, out_features=1728, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=576, out_features=576, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=576, out_features=2304, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=2304, out_features=576, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_trans_6): ConvTransBlock(\n",
       "    (cnn_block): ConvBlock(\n",
       "      (conv1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(192, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(192, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(768, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (fusion_block): ConvBlock(\n",
       "      (conv1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(192, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(192, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(768, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (squeeze_block): FCUDown(\n",
       "      (conv_project): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sample_pooling): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      (ln): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (act): GELU(approximate='none')\n",
       "    )\n",
       "    (expand_block): FCUUp(\n",
       "      (conv_project): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(192, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (trans_block): Block(\n",
       "      (norm1): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=576, out_features=1728, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=576, out_features=576, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=576, out_features=2304, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=2304, out_features=576, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_trans_7): ConvTransBlock(\n",
       "    (cnn_block): ConvBlock(\n",
       "      (conv1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(192, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(192, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(768, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (fusion_block): ConvBlock(\n",
       "      (conv1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(192, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(192, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(768, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (squeeze_block): FCUDown(\n",
       "      (conv_project): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sample_pooling): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      (ln): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (act): GELU(approximate='none')\n",
       "    )\n",
       "    (expand_block): FCUUp(\n",
       "      (conv_project): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(192, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (trans_block): Block(\n",
       "      (norm1): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=576, out_features=1728, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=576, out_features=576, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=576, out_features=2304, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=2304, out_features=576, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_trans_8): ConvTransBlock(\n",
       "    (cnn_block): ConvBlock(\n",
       "      (conv1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(192, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(192, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(768, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (fusion_block): ConvBlock(\n",
       "      (conv1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(192, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(192, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(768, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (squeeze_block): FCUDown(\n",
       "      (conv_project): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sample_pooling): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      (ln): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (act): GELU(approximate='none')\n",
       "    )\n",
       "    (expand_block): FCUUp(\n",
       "      (conv_project): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(192, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (trans_block): Block(\n",
       "      (norm1): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=576, out_features=1728, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=576, out_features=576, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=576, out_features=2304, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=2304, out_features=576, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_trans_9): ConvTransBlock(\n",
       "    (cnn_block): ConvBlock(\n",
       "      (conv1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(384, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(384, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1536, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "      (residual_conv): Conv2d(768, 1536, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (residual_bn): BatchNorm2d(1536, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (fusion_block): ConvBlock(\n",
       "      (conv1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(384, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(384, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1536, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (squeeze_block): FCUDown(\n",
       "      (conv_project): Conv2d(384, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sample_pooling): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
       "      (ln): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (act): GELU(approximate='none')\n",
       "    )\n",
       "    (expand_block): FCUUp(\n",
       "      (conv_project): Conv2d(576, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(384, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (trans_block): Block(\n",
       "      (norm1): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=576, out_features=1728, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=576, out_features=576, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=576, out_features=2304, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=2304, out_features=576, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_trans_10): ConvTransBlock(\n",
       "    (cnn_block): ConvBlock(\n",
       "      (conv1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(384, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(384, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1536, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (fusion_block): ConvBlock(\n",
       "      (conv1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(384, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(384, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1536, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (squeeze_block): FCUDown(\n",
       "      (conv_project): Conv2d(384, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sample_pooling): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
       "      (ln): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (act): GELU(approximate='none')\n",
       "    )\n",
       "    (expand_block): FCUUp(\n",
       "      (conv_project): Conv2d(576, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(384, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (trans_block): Block(\n",
       "      (norm1): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=576, out_features=1728, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=576, out_features=576, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=576, out_features=2304, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=2304, out_features=576, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_trans_11): ConvTransBlock(\n",
       "    (cnn_block): ConvBlock(\n",
       "      (conv1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(384, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(384, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1536, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (fusion_block): ConvBlock(\n",
       "      (conv1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(384, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(384, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1536, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (squeeze_block): FCUDown(\n",
       "      (conv_project): Conv2d(384, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sample_pooling): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
       "      (ln): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (act): GELU(approximate='none')\n",
       "    )\n",
       "    (expand_block): FCUUp(\n",
       "      (conv_project): Conv2d(576, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(384, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (trans_block): Block(\n",
       "      (norm1): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=576, out_features=1728, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=576, out_features=576, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=576, out_features=2304, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=2304, out_features=576, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_trans_12): ConvTransBlock(\n",
       "    (cnn_block): ConvBlock(\n",
       "      (conv1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(384, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(384, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1536, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (fusion_block): ConvBlock(\n",
       "      (conv1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(384, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(384, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1536, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "      (residual_conv): Conv2d(1536, 1536, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (residual_bn): BatchNorm2d(1536, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (squeeze_block): FCUDown(\n",
       "      (conv_project): Conv2d(384, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sample_pooling): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
       "      (ln): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (act): GELU(approximate='none')\n",
       "    )\n",
       "    (expand_block): FCUUp(\n",
       "      (conv_project): Conv2d(576, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(384, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (trans_block): Block(\n",
       "      (norm1): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=576, out_features=1728, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=576, out_features=576, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=576, out_features=2304, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=2304, out_features=576, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def trainVal(model, criterion, optimizer, num_epochs, min_val_loss, train_loader, val_loader, device):\n",
    "    best_acc = 0.0\n",
    "    min_loss = min_val_loss\n",
    "\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Using tqdm for progress tracking\n",
    "        for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch}', leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accs.append(epoch_acc)\n",
    "        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluate mode\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(val_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(val_loader.dataset)\n",
    "\n",
    "        val_losses.append(epoch_loss)\n",
    "        val_accs.append(epoch_acc)\n",
    "        print(f'Val Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        # Update the learning rate\n",
    "        # scheduler.step()  # Uncomment if using a learning rate scheduler\n",
    "\n",
    "        # Save the model if it has the best validation accuracy so far\n",
    "        # if epoch_acc > best_acc:\n",
    "        #     best_acc = epoch_acc\n",
    "        #     state = {\n",
    "        #         'epoch': epoch + 1,\n",
    "        #         'state_dict': model.state_dict(),\n",
    "        #         'optimizer': optimizer.state_dict(),\n",
    "        #         'min_loss': epoch_loss\n",
    "        #     }\n",
    "        # torch.save(state, 'weight/cvit_deepfake_detection_v2.pth')\n",
    "\n",
    "    return train_losses, train_accs, val_losses, val_accs, min_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/39\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "max() received an invalid combination of arguments - got (list, int), but expected one of:\n * (Tensor input, *, Tensor out)\n * (Tensor input, Tensor other, *, Tensor out)\n * (Tensor input, int dim, bool keepdim, *, tuple of Tensors out)\n * (Tensor input, name dim, bool keepdim, *, tuple of Tensors out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m min_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Call the training function with the appropriate data loaders\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train_losses, train_accs, val_losses, val_accs, min_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainVal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_val_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 31\u001b[0m, in \u001b[0;36mtrainVal\u001b[0;34m(model, criterion, optimizer, num_epochs, min_val_loss, train_loader, val_loader, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     30\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m---> 31\u001b[0m     _, preds \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# backward + optimize only if in training phase\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: max() received an invalid combination of arguments - got (list, int), but expected one of:\n * (Tensor input, *, Tensor out)\n * (Tensor input, Tensor other, *, Tensor out)\n * (Tensor input, int dim, bool keepdim, *, tuple of Tensors out)\n * (Tensor input, name dim, bool keepdim, *, tuple of Tensors out)\n"
     ]
    }
   ],
   "source": [
    "# Define the initial minimum validation loss\n",
    "min_val_loss = float('inf')\n",
    "\n",
    "# Call the training function with the appropriate data loaders\n",
    "train_losses, train_accs, val_losses, val_accs, min_loss = trainVal(\n",
    "    model, criterion, optimizer, num_epochs, min_val_loss, train_loader, val_loader, device\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graspenv",
   "language": "python",
   "name": "graspenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
